{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookId":"9f440908-d7b4-4b4a-93b0-550f9535c110","language_info":{"file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.7.7"},"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"}},"cells":[{"cell_type":"markdown","source":"Adapted from https://colab.research.google.com/drive/1CSy-xfrnTX28p1difoTA8ulYw0zytJkq#scrollTo=srZU0YiAQ8rm","metadata":{"cellId":"8iw30x1kwp2lagasjzsq"}},{"cell_type":"code","source":"#!g1.1\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax.experimental.ode import odeint\nimport matplotlib.pyplot as plt\nfrom functools import partial # reduces arguments to function by making some subset implicit\n\nfrom jax.experimental import stax\nfrom jax.experimental import optimizers\n\n# visualization\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\n#from moviepy.editor import ImageSequenceClip\nfrom functools import partial\n#import proglog\nfrom PIL import Image","metadata":{"execution_id":"71596a81-10a9-4185-a74b-d4ccdbfa2d5f","cellId":"g6lj0i8gc04cvn4bhozthn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nfrom jax.lib import xla_bridge\nprint(xla_bridge.get_backend().platform)","metadata":{"execution_id":"b523b334-c3bc-4170-afe3-ef150a5f6198","cellId":"jakdmre7my9bjujtqb20rc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ndef lagrangian(q, q_dot, m1, m2, l1, l2, g):\n  t1, t2 = q     # theta 1 and theta 2\n  w1, w2 = q_dot # omega 1 and omega 2\n\n  # kinetic energy (T)\n  T1 = 0.5 * m1 * (l1 * w1)**2\n  T2 = 0.5 * m2 * ((l1 * w1)**2 + (l2 * w2)**2 +\n                    2 * l1 * l2 * w1 * w2 * jnp.cos(t1 - t2))\n  T = T1 + T2\n  \n  # potential energy (V)\n  y1 = -l1 * jnp.cos(t1)\n  y2 = y1 - l2 * jnp.cos(t2)\n  V = m1 * g * y1 + m2 * g * y2\n\n  return T - V","metadata":{"execution_id":"bb3e9f5d-c8a0-4e0d-ae74-c3062ac9f781","cellId":"q7q6ltvbktgr8zbk9na2y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ndef f_analytical(state, t=0, m1=1, m2=1, l1=1, l2=1, g=9.8):\n  t1, t2, w1, w2 = state\n  a1 = (l2 / l1) * (m2 / (m1 + m2)) * jnp.cos(t1 - t2)\n  a2 = (l1 / l2) * jnp.cos(t1 - t2)\n  f1 = -(l2 / l1) * (m2 / (m1 + m2)) * (w2**2) * jnp.sin(t1 - t2) - \\\n      (g / l1) * jnp.sin(t1)\n  f2 = (l1 / l2) * (w1**2) * jnp.sin(t1 - t2) - (g / l2) * jnp.sin(t2)\n  g1 = (f1 - a1 * f2) / (1 - a1 * a2)\n  g2 = (f2 - a2 * f1) / (1 - a1 * a2)\n  return jnp.stack([w1, w2, g1, g2])","metadata":{"execution_id":"79c54665-fe7d-4806-82f9-2375658e9c67","cellId":"jt0i67a8rs3j5ut9699qk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ndef equation_of_motion(lagrangian, state, t=None):\n  q, q_t = jnp.split(state, 2)\n  q_tt = (jnp.linalg.pinv(jax.hessian(lagrangian, 1)(q, q_t))\n          @ (jax.grad(lagrangian, 0)(q, q_t)\n             - jax.jacobian(jax.jacobian(lagrangian, 1), 0)(q, q_t) @ q_t))\n  return jnp.concatenate([q_t, q_tt])\n\ndef solve_lagrangian(lagrangian, initial_state, **kwargs):\n  # We currently run odeint on CPUs only, because its cost is dominated by\n  # control flow, which is slow on GPUs.\n  @partial(jax.jit, backend='cpu')\n  def f(initial_state):\n    return odeint(partial(equation_of_motion, lagrangian),\n                  initial_state, **kwargs)\n  return f(initial_state)","metadata":{"execution_id":"937778c5-044a-43fb-a499-2c9481e724c9","cellId":"2ppe5qhlaru23jhnwj4e8rg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n# Double pendulum dynamics via the rewritten Euler-Lagrange\n@partial(jax.jit, backend='cpu')\ndef solve_autograd(initial_state, times, m1=1, m2=1, l1=1, l2=1, g=9.8):\n  L = partial(lagrangian, m1=m1, m2=m2, l1=l1, l2=l2, g=g)\n  return solve_lagrangian(L, initial_state, t=times, rtol=1e-10, atol=1e-10)\n\n# Double pendulum dynamics via analytical forces taken from Diego's blog\n@partial(jax.jit, backend='cpu')\ndef solve_analytical(initial_state, times):\n  return odeint(f_analytical, initial_state, t=times, rtol=1e-10, atol=1e-10)\n\ndef normalize_dp(state):\n  # wrap generalized coordinates to [-pi, pi]\n  return jnp.concatenate([(state[:2] + np.pi) % (2 * np.pi) - np.pi, state[2:]])\n\ndef rk4_step(f, x, t, h):\n  # one step of runge-kutta integration\n  k1 = h * f(x, t)\n  k2 = h * f(x + k1/2, t + h/2)\n  k3 = h * f(x + k2/2, t + h/2)\n  k4 = h * f(x + k3, t + h)\n  return x + 1/6 * (k1 + 2 * k2 + 2 * k3 + k4)","metadata":{"execution_id":"6f912be4-025b-4887-88d8-d4b415325b69","cellId":"dx4td8mbvtrirwcw0oswxh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n# choose an initial state\nx0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\nnoise = np.random.RandomState(0).randn(x0.size)\nt = np.linspace(0, 40, num=401, dtype=np.float32)","metadata":{"execution_id":"640d4ce7-07c7-4fd3-9b8b-4b2b0134e4b4","cellId":"88ia1y56otu4vd29wmqo8k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n# compute dynamics analytically\n%time x_analytical = jax.device_get(solve_analytical(x0, t))\nnoise_coeff_1, noise_coeff_2 = 1e-10, 1e-11\nx_perturbed_1 = jax.device_get(solve_analytical(x0 + noise_coeff_1 * noise, t))\nx_perturbed_2 = jax.device_get(solve_analytical(x0 + noise_coeff_2 * noise, t))","metadata":{"execution_id":"d4f75f9d-8a45-4d49-a1a9-bee9b26ba135","cellId":"vykj4juuupkbcxhkyf1tz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n# compute dynamics via autograd / the rewritten E-L equation\n%time x_autograd = jax.device_get(solve_autograd(x0, t))","metadata":{"execution_id":"32419203-c094-4cab-b700-5c489ee13c7a","cellId":"35rdwkep7xs61pyzhte7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nplt.figure(figsize=[10,3], dpi=120) ; plt.xlim(0, 100)\nplt.subplot(1,3,1)\nplt.title(\"Analytic vs Perturbed ($\\epsilon$={})\".format(noise_coeff_1))\nplt.xlabel(\"Time\") ; plt.ylabel(\"State\")\nplt.plot(t, x_analytical[:, 0], 'g-', label='$q$')\nplt.plot(t, x_analytical[:, 1], 'c-', label='$\\dot q$')\nplt.plot(t, x_perturbed_1[:, 0], 'g--', label='pert. $q$')\nplt.plot(t, x_perturbed_1[:, 1], 'c--', label='pert. $\\dot q$')\nplt.legend(fontsize=6)\n\nplt.subplot(1,3,2)\nplt.title(\"Analytic vs Perturbed ($\\epsilon$={})\".format(noise_coeff_2))\nplt.xlabel(\"Time\") ; plt.ylabel(\"State\")\nplt.plot(t, x_analytical[:, 0], 'g-', label='$q$')\nplt.plot(t, x_analytical[:, 1], 'c-', label='$\\dot q$')\nplt.plot(t, x_perturbed_2[:, 0], 'g--', label='pert. $q$')\nplt.plot(t, x_perturbed_2[:, 1], 'c--', label='pert. $\\dot q$')\nplt.legend(fontsize=6)\n\nplt.subplot(1,3,3)\nplt.title(\"Analytic vs Autograd\")\nplt.xlabel(\"Time\") ; plt.ylabel(\"State\")\nplt.plot(t, x_analytical[:, 0], 'g-', label='$q$')\nplt.plot(t, x_analytical[:, 1], 'c-', label='$\\dot q$')\nplt.plot(t, x_autograd[:, 0], 'g--', label='autograd $q$')\nplt.plot(t, x_autograd[:, 1], 'c--', label='autograd $\\dot q$')\nplt.legend(fontsize=6)\n\nplt.tight_layout() ; plt.show()","metadata":{"execution_id":"7ed6260c-6c71-4c88-a488-270721058449","cellId":"cv9yqn6hlffmt8hbxala3k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ntime_step = 0.01\nN = 1500\nanalytical_step = jax.jit(jax.vmap(partial(rk4_step, f_analytical, t=0.0, h=time_step)))\n\n# x0 = np.array([-0.3*np.pi, 0.2*np.pi, 0.35*np.pi, 0.5*np.pi], dtype=np.float32)\nx0 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\nt = np.arange(N, dtype=np.float32) # time steps 0 to N\n%time x_train = jax.device_get(solve_analytical(x0, t)) # dynamics for first N time steps\n%time xt_train = jax.device_get(jax.vmap(f_analytical)(x_train)) # time derivatives of each state\n%time y_train = jax.device_get(analytical_step(x_train)) # analytical next step\n\nnoise = np.random.RandomState(0).randn(x0.size)\nt_test = np.arange(N, 2*N, dtype=np.float32) # time steps N to 2N\n%time x_test = jax.device_get(solve_analytical(x0, t_test)) # dynamics for next N time steps\n%time xt_test = jax.device_get(jax.vmap(f_analytical)(x_test)) # time derivatives of each state\n%time y_test = jax.device_get(analytical_step(x_test)) # analytical next step","metadata":{"execution_id":"c0257785-bb37-467a-b5fb-53684dbd9370","cellId":"dey4qz3tbguqgfihywi7o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n# preprocess\ntrain_vis = jax.vmap(normalize_dp)(x_train)\ntest_vis = jax.vmap(normalize_dp)(x_test)\n\nvel_angle = lambda data:  (np.arctan2(data[:,3], data[:,2]) / np.pi + 1)/2\nvel_color = lambda vangle: np.stack( [np.zeros_like(vangle), vangle, 1-vangle]).T\ntrain_colors = vel_color(vel_angle(train_vis))\ntest_colors = vel_color(vel_angle(test_vis))\n\n# plot\nSCALE = 80 ; WIDTH = 0.006\nplt.figure(figsize=[8,4], dpi=120)\nplt.subplot(1,2,1)\nplt.title(\"Train data\") ; plt.xlabel(r'$\\theta_1$') ; plt.ylabel(r'$\\theta_2$')\nplt.quiver(*train_vis.T, color=train_colors, scale=SCALE, width=WIDTH)\n\nplt.subplot(1,2,2)\nplt.title(\"Test data\") ; plt.xlabel(r'$\\theta_1$') ; plt.ylabel(r'$\\theta_2$')\nplt.quiver(*test_vis.T, color=test_colors, scale=SCALE, width=WIDTH)\n\nplt.tight_layout() ; plt.show()","metadata":{"execution_id":"83b84dbc-99df-42bb-aa71-98e9961f14de","cellId":"h0m8ppfppl6r92mswewcus","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n# replace the lagrangian with a parameteric model\ndef learned_lagrangian(params):\n  def lagrangian(q, q_t):\n    assert q.shape == (2,)\n    state = normalize_dp(jnp.concatenate([q, q_t]))\n    return jnp.squeeze(nn_forward_fn(params, state), axis=-1)\n  return lagrangian\n\n# define the loss of the model (MSE between predicted q, \\dot q and targets)\n@jax.jit\ndef loss(params, batch, time_step=None):\n  state, targets = batch\n  if time_step is not None:\n    f = partial(equation_of_motion, learned_lagrangian(params))\n    preds = jax.vmap(partial(rk4_step, f, t=0.0, h=time_step))(state)\n  else:\n    preds = jax.vmap(partial(equation_of_motion, learned_lagrangian(params)))(state)\n  return jnp.mean((preds - targets) ** 2)\n\n# build a neural network model\ninit_random_params, nn_forward_fn = stax.serial(\n    stax.Dense(128),\n    stax.Softplus,\n    stax.Dense(128),\n    stax.Softplus,\n    stax.Dense(1),\n)","metadata":{"execution_id":"81c209e6-5093-43b0-a07d-33f30844d714","cellId":"xq2dyafi48co73dlh9c4an","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n@jax.jit\ndef update_timestep(i, opt_state, batch):\n  params = get_params(opt_state)\n  return opt_update(i, jax.grad(loss)(params, batch, time_step), opt_state)\n\n@jax.jit\ndef update_derivative(i, opt_state, batch):\n  params = get_params(opt_state)\n  return opt_update(i, jax.grad(loss)(params, batch, None), opt_state)\n\nx_train = jax.device_put(jax.vmap(normalize_dp)(x_train))\ny_train = jax.device_put(y_train)\n\nx_test = jax.device_put(jax.vmap(normalize_dp)(x_test))\ny_test = jax.device_put(y_test)","metadata":{"execution_id":"a679e879-f4b6-4b2c-92cc-0d4c71f0a234","cellId":"kwk1fqli3sflznarzzx6i","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n%%time\n\nrng = jax.random.PRNGKey(0)\n_, init_params = init_random_params(rng, (-1, 4))\n\n# numbers in comments denote stephan's settings\nbatch_size = 100\ntest_every = 10\nnum_batches = 1500\n\ntrain_losses = []\ntest_losses = []\n\n# adam w learn rate decay\nopt_init, opt_update, get_params = optimizers.adam(\n    lambda t: jnp.select([t < batch_size*(num_batches//3),\n                          t < batch_size*(2*num_batches//3),\n                          t > batch_size*(2*num_batches//3)],\n                         [1e-3, 3e-4, 1e-4]))\nopt_state = opt_init(init_params)\n\nfor iteration in range(batch_size*num_batches + 1):\n  if iteration % batch_size == 0:\n    params = get_params(opt_state)\n    train_loss = loss(params, (x_train, xt_train))\n    train_losses.append(train_loss)\n    test_loss = loss(params, (x_test, xt_test))\n    test_losses.append(test_loss)\n    if iteration % (batch_size*test_every) == 0:\n      print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\n  opt_state = update_derivative(iteration, opt_state, (x_train, xt_train))\n\nparams = get_params(opt_state)","metadata":{"execution_id":"6d60804f-4390-43aa-897f-9e2a7ba941bc","cellId":"sprnba05iclldep4tybtj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nplt.figure(figsize=(8, 3.5), dpi=120)\nplt.plot(train_losses, label='Train loss')\nplt.plot(test_losses, label='Test loss')\nplt.yscale('log')\nplt.ylim(None, 200)\nplt.title('Losses over training')\nplt.xlabel(\"Train step\") ; plt.ylabel(\"Mean squared error\")\nplt.legend() ; plt.show()","metadata":{"execution_id":"29d1e62c-4020-4fc0-bd34-08f5707ac4a9","cellId":"qvb1hie6ywcmwcqpieznnh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nxt_pred = jax.vmap(partial(equation_of_motion, learned_lagrangian(params)))(x_test)\n\nfig, axes = plt.subplots(1, 2, figsize=(6, 3), dpi=120)\naxes[0].scatter(xt_test[:, 2], xt_pred[:, 2], s=6, alpha=0.2)\naxes[0].set_title('Predicting $\\dot q$')\naxes[0].set_xlabel('$\\dot q$ actual')\naxes[0].set_ylabel('$\\dot q$ predicted')\naxes[1].scatter(xt_test[:, 3], xt_pred[:, 3], s=6, alpha=0.2)\naxes[1].set_title('Predicting $\\ddot q$')\naxes[1].set_xlabel('$\\ddot q$ actual')\naxes[1].set_ylabel('$\\ddot q$ predicted')\nplt.tight_layout()","metadata":{"execution_id":"f48ff14e-e919-4da1-bceb-036226a47f6b","cellId":"hgtcwz82226ac9ndyfmixf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n# make a meshgrid of possible angles\nangle_range = -np.pi + 2 * np.pi * np.arange(100)/100\nx_scanned = np.stack(np.meshgrid(angle_range, angle_range, [0.0], [0.0], indexing='ij'), axis=-1).reshape(-1, 4)","metadata":{"execution_id":"b78acbc1-2d7b-4fc8-bb4d-93351d2fa388","cellId":"3si5kwcqahg9834ybrjukw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n%time xt_actual = jax.vmap(f_analytical)(x_scanned) # actual time derivatives","metadata":{"execution_id":"2054afe2-8a42-484d-ba2c-a4dd6f10a704","cellId":"evpdyspqkhldmkwtm9c6g","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n%time xt_predicted = jax.vmap(partial(equation_of_motion, learned_lagrangian(params)))(x_scanned) # predicted time derivatives","metadata":{"execution_id":"7185cf47-1361-4923-bc95-230b451fa7e0","cellId":"aggs8avn2xkuo6ekm0lnwr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\nimport xarray\ndataarray = xarray.DataArray(np.stack([xt_actual.reshape(100, 100, 4),\n                                       xt_predicted.reshape(100, 100, 4)]),\n                             dims=['model', 'theta1', 'theta2', 'time_derivative'],\n                             coords={'model': ['actual', 'predicted'], 'theta1': angle_range, 'theta2': angle_range})","metadata":{"execution_id":"b5259ea2-377a-4283-8b2a-d32c319c5bcd","cellId":"teppe3i7qxlcuz5y5cnivj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ndataarray.isel(time_derivative=2).plot.contourf(levels=31, col='model', x='theta1', y='theta2', vmin=-10, vmax=10)\nprint(\"Colorscale is \\dot theta1\")\nplt.show()","metadata":{"execution_id":"4efaedaf-c17f-4ced-b8ee-7cb3ce50bdd1","cellId":"g10yvrszlufe8lelw15cc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\ndataarray.isel(time_derivative=3).plot.contourf(levels=31, col='model', x='theta1', y='theta2', vmin=-10, vmax=10)\nprint(\"Colorscale is \\dot theta2\")\nplt.show()","metadata":{"execution_id":"1b82a826-0df6-4e84-95f6-5d271883e25b","cellId":"m5jbbxj7bi2oz7sf9tfdf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# choose an initial state\nx1 = np.array([3*np.pi/7, 3*np.pi/4, 0, 0], dtype=np.float32)\n\nt2 = np.linspace(0, 20, num=301)\n%time x1_analytical = jax.device_get(solve_analytical(x1, t2))\nnoise_coeff_3 = 1e-3\n%time x1_perturbed = jax.device_get(solve_analytical(x1 + noise_coeff_3 * noise, t2))\n%time x1_model = jax.device_get(solve_lagrangian(learned_lagrangian(params), x1, t=t2))","metadata":{"cellId":"8n3qmrzpiksi2ayk28hfa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=[6,6], dpi=120) ; plt.xlim(0, 20)\nplt.subplot(2,1,1)\nplt.title(\"Analytic vs Perturbed ($\\epsilon$={})\".format(noise_coeff_3))\nplt.xlabel(\"Time\") ; plt.ylabel(\"State\")\nplt.plot(t2, x1_analytical[:, 2], 'g-', label=r'$\\theta_1$ exact')\nplt.plot(t2, x1_analytical[:, 3], 'c-', label=r'$\\theta_2$ exact')\nplt.plot(t2, x1_perturbed[:, 2], 'g--', label=r'$\\theta_1$ perturbed')\nplt.plot(t2, x1_perturbed[:, 3], 'c--', label=r'$\\theta_2$ perturbed')\nplt.legend(fontsize=6)\n\nplt.subplot(2,1,2)\nplt.title(\"Analytic vs LNN\")\nplt.xlabel(\"Time\") ; plt.ylabel(\"State\")\nplt.plot(t2, x1_analytical[:, 2], 'g-', label=r'$\\theta_1$ exact')\nplt.plot(t2, x1_analytical[:, 3], 'c-', label=r'$\\theta_2$ exact')\nplt.plot(t2, x1_model[:, 2], 'g--', label=r'$\\theta_1$ LNN')\nplt.plot(t2, x1_model[:, 3], 'c--', label=r'$\\theta_2$ LNN')\nplt.legend(fontsize=6)\n\nplt.tight_layout() ; plt.show()","metadata":{"cellId":"me3qwjdtmej23k1m5xsguk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!g1.1\n","metadata":{"cellId":"hx9kryt8voezrakr9lse0a","trusted":true},"outputs":[],"execution_count":null}]}